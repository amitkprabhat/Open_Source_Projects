{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMUPZETHNIQfeIvpg+qjx2v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cVHo6h3uXbMa","executionInfo":{"status":"ok","timestamp":1695565640721,"user_tz":-330,"elapsed":113374,"user":{"displayName":"Amit Kumar Prabhat","userId":"16842616251246087913"}},"outputId":"d29db1f2-fd19-4394-cfd7-bb2b99d186c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","class DataGenerator(Dataset):\n","\tdef __init__(self, path):\n","\t\tself.files = self.get_files(path)\n","\tdef __len__(self):\n","\t\treturn len(self.files)\n","\tdef __getitem__(self,idx):\n","\t\treturn data, cap\n","\n","def load_data(data_path, batch_size=1, num_workers=10, shuffle=True):\n","\n","\tdataset = DataGenerator(data_path)\n","\tdata_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n","\n","\treturn data_loader"],"metadata":{"id":"paJC8IClX6YA","executionInfo":{"status":"ok","timestamp":1695565643976,"user_tz":-330,"elapsed":470,"user":{"displayName":"Amit Kumar Prabhat","userId":"16842616251246087913"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import random\n","from torch.utils.data import Dataset, DataLoader\n","import cv2\n","import os\n","import re\n","import torch\n","import numpy as np\n","\n","use_cuda = torch.cuda.is_available()\n","print('use_cuda: {}'.format(use_cuda))\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","class DataGenerator(Dataset):\n","    def __init__(self, data, word2int):\n","        self.files = data\n","        self.word2int = word2int\n","\n","    def __len__(self):\n","        return len(self.files)\n","    def __getitem__(self,idx):\n","        img_path, cap = self.files[idx]\n","        img = cv2.imread(img_path)\n","        img = cv2.resize(img, (224, 224))\n","        return img, cap\n","\n","    def collate_fn_customised(self, data):\n","\n","        img_list = []\n","        cap_list = []\n","        for i, c in data:\n","            img_list.append(i)\n","            cap_list.append(c)\n","\n","        ##OPERATING ON CAPTION\n","        modified_cap = []\n","        for cap in cap_list:\n","            #Removing Functuatoons and Numbers and converting it into lower case and breaking it into tokens.\n","            cap = re.sub('[^a-zA-Z ]+', '', cap.lower()).split()\n","            #Added BOS and EOS to the start and the end of the sent. respectively.\n","            cap = [\"<BOS>\"] + cap + [\"<EOS>\"]\n","            modified_cap.append(cap)\n","        max_len = max([len(cap) for cap in modified_cap])\n","        cap_list = []\n","        for cap in modified_cap:\n","            while(len(cap)<max_len):\n","                cap = cap + [\"<PAD>\"]\n","            cap_list.append(cap)\n","        modified_cap = []\n","        for cap in cap_list:\n","            numericalizaed_sentences = []\n","            for word in cap:\n","                try:\n","                    numericalizaed_sentences.append(self.word2int[word])\n","                except:\n","                    numericalizaed_sentences.append(self.word2int[\"<UNK>\"])\n","            modified_cap.append(numericalizaed_sentences)\n","\n","        cap_array = np.array(modified_cap)\n","        cap_tensor = torch.tensor(cap_array)\n","        img_array = np.array(img_list)\n","        img_tensor = torch.tensor(img_array)\n","\n","        return img_tensor, cap_tensor\n","\n","def load_data(data, word2int, batch_size=1, num_workers=10, shuffle=True):\n","\n","\tdataset = DataGenerator(data, word2int)\n","\tdata_loader = DataLoader(dataset, collate_fn= dataset.collate_fn_customised, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n","\n","\treturn data_loader\n","\n","def train_val_test_split(img_folder, annotation_path):\n","\n","    lines = open(annotation_path, \"r\").readlines()\n","    lines = lines[1:]\n","\n","    #Populated a dictionary in the format of key=img and value = [cap1, cap2, ...]\n","    image_caption_dict = {}\n","    for l in lines:\n","        img = l.split(',')[0]\n","        cap = l.split(',')[1][:-1]\n","        if img in image_caption_dict.keys():\n","            image_caption_dict[img].append(cap)\n","        else:\n","            image_caption_dict[img] = [cap]\n","\n","    images = list(set([l[:-1].split(',')[0] for l in lines]))\n","    random.seed(10)\n","    random.shuffle(images)\n","\n","    #Splitting the list of images into 70, 10, 20 % split in train, val, test respectively.\n","    train_images = images[:int(0.7*len(images))]\n","    val_images = images[int(0.7*len(images)):int(0.8*len(images))]\n","    test_images = images[int(0.8*len(images)):]\n","\n","    train_list = [(os.path.join(img_folder, img), cap) for img in train_images for cap in image_caption_dict[img] if os.path.exists(os.path.join(img_folder, img))]\n","    val_list = [(os.path.join(img_folder, img), cap) for img in val_images for cap in image_caption_dict[img] if os.path.exists(os.path.join(img_folder, img))]\n","    test_list = [(os.path.join(img_folder, img), cap) for img in test_images for cap in image_caption_dict[img] if os.path.exists(os.path.join(img_folder, img))]\n","\n","    return train_list, val_list, test_list\n","\n","\n","def train_epoch(train_loader):\n","\n","     for img_tensor, cap_tensor in train_loader:\n","        img_tensor = img_tensor.to(device)\n","        cap_tensor = cap_tensor.to(device)\n","\n","        print(img_tensor.shape, cap_tensor.shape)\n","\n","def val_epoch():\n","    pass\n","def test_epoch():\n","    pass\n","\n","def train_val_test(train_loader):\n","\n","    train_epoch(train_loader)\n","\n","def main():\n","\n","    data_folder = \"/content/drive/MyDrive/data\"\n","    image_folder = os.path.join(data_folder, \"flickr30k-images\")\n","    #data_folder = \"/content/drive/MyDrive/projects/flkr_image_capt\"\n","    #image_folder = os.path.join(data_folder, \"Images\")\n","    #cap_folder = \"/content/drive/MyDrive/projects/flkr_image_capt\"\n","    #annotation_path = os.path.join(cap_folder, \"captions.txt\")\n","    annotation_path = os.path.join(data_folder, \"captions_30k.txt\")\n","    train_list, val_list, test_list = train_val_test_split(image_folder, annotation_path)\n","\n","    #Preparing Dictionary\n","    words = []\n","    for _, cap in train_list:\n","        cap = re.sub('[^a-zA-Z ]+', '', cap.lower()).split()\n","        words.extend(cap)\n","    words = list(set(words))\n","    words.sort()\n","    words = [\"<BOS>\", \"<EOS>\", \"<PAD>\", \"<UNK>\"] + words\n","    word2int = {}\n","    int2word = {}\n","    for id, word in enumerate(words):\n","        word2int[word] = id\n","        int2word[id] = word\n","\n","    train_loader = load_data(val_list, word2int, batch_size=128, num_workers=1)\n","\n","    train_val_test(train_loader)\n","\n","main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U_t85aG6YYE7","executionInfo":{"status":"ok","timestamp":1695567252912,"user_tz":-330,"elapsed":870204,"user":{"displayName":"Amit Kumar Prabhat","userId":"16842616251246087913"}},"outputId":"f45a9366-5028-4d09-94ff-d43f9bfd7363"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["use_cuda: False\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 56])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 33])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 30])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 44])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 35])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 27])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 24])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 31])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 38])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 36])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 33])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 48])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 39])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 39])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 27])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 36])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 34])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 41])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 28])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 33])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 43])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 40])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 27])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 34])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 46])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 36])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 27])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 30])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 30])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 36])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 42])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 46])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 37])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 25])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 47])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 34])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 36])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 28])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 33])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 37])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 37])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 41])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 35])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 31])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 39])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 70])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 41])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 38])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 36])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 41])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 33])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 37])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 31])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 67])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 32])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 32])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 39])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 33])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 33])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 36])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 41])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 31])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 32])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 35])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 37])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 32])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 42])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 36])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 37])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 41])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 42])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 38])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 35])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 38])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 49])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 39])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 52])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 32])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 29])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 36])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 38])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 29])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 36])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 40])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 25])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 55])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 47])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 37])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 37])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 41])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 28])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 32])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 31])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 34])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 42])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 42])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 37])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 44])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 28])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 33])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 39])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 38])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 35])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 57])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 40])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 44])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 27])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 27])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 29])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 31])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 31])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 40])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 34])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 37])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 33])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 34])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 35])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 32])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 41])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 75])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 48])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 36])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 29])\n","torch.Size([128, 224, 224, 3]) torch.Size([128, 30])\n","torch.Size([18, 224, 224, 3]) torch.Size([18, 21])\n"]}]}]}